{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccmjbDMuA_q7",
        "outputId": "aff2a893-2884-4569-94e8-6d3a7056dd60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sacrebleu in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (2.5.1)\n",
            "Requirement already satisfied: datasets in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (4.3.0)\n",
            "Requirement already satisfied: transformers in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (4.57.1)\n",
            "Requirement already satisfied: SentencePiece in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (0.2.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: portalocker in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from sacrebleu) (3.2.0)\n",
            "Requirement already satisfied: regex in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from sacrebleu) (2025.10.23)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from sacrebleu) (2.3.4)\n",
            "Requirement already satisfied: colorama in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from sacrebleu) (6.0.2)\n",
            "Requirement already satisfied: filelock in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
            "Requirement already satisfied: pandas in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
            "Requirement already satisfied: httpx<1.0.0 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
            "Collecting click (from sacremoses)\n",
            "  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: joblib in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from sacremoses) (1.5.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: anyio in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/ruksad/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hUsing cached click-8.3.0-py3-none-any.whl (107 kB)\n",
            "Installing collected packages: click, sacremoses\n",
            "Successfully installed click-8.3.0 sacremoses-0.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sacrebleu datasets transformers SentencePiece sacremoses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q85mhytyMfS"
      },
      "source": [
        "BLEU measures n-gram overlap between:\n",
        "\n",
        "The machine-generated translation (called hypothesis), and\n",
        "\n",
        "The reference translation(s) (human-annotated ground truth).\n",
        "\n",
        "It checks how many words, word pairs, triplets, etc. match between them.\n",
        "\n",
        "Reference:  The cat is on the mat\n",
        "Hypothesis: The cat sat on the mat\n",
        "\n",
        "Unigrams (n=1):\n",
        "\n",
        "Overlap words: “The”, “cat”, “on”, “the”, “mat” → 5 matches out of 6 in hypothesis\n",
        "\n",
        "Bigrams (n=2):\n",
        "\n",
        "Reference: \"The cat\", \"cat is\", \"is on\", \"on the\", \"the mat\"\n",
        "\n",
        "Hypothesis: \"The cat\", \"cat sat\", \"sat on\", \"on the\", \"the mat\"\n",
        "\n",
        "Overlaps: \"The cat\", \"on the\", \"the mat\" → 3 matches out of 5\n",
        "\n",
        "Apply brevity penalty (BP)\n",
        "\n",
        "If the hypothesis is too short, it gets penalized.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "78czNQihDP5f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import sacrebleu\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Config\n",
        "\n",
        "MODEL_ID = \"Helsinki-NLP/opus-mt-en-hi\"\n",
        "DATASET_ID = \"cfilt/iitb-english-hindi\"\n",
        "SPLIT = \"test\"\n",
        "\n",
        "# Full test evaluation\n",
        "EVAL_SIZE = None\n",
        "BATCH_SIZE = 64\n",
        "MAX_NEW_TOKENS = 128\n",
        "NUM_BEAMS = 6\n",
        "LENGTH_PENALTY = 1.1     #Adjusts how much the model is penalized for producing long sequences in beam search.\n",
        "NO_REPEAT_NGRAM = 2   #Prevents the model from repeating the same 3-word sequence during generation\n",
        "SEED = 42\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "USE_FP16 = DEVICE == \"cuda\"   # half-precision on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQaRqy87D5yD",
        "outputId": "70bd19c0-5cb4-4f2e-d4ff-e558288582df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: Helsinki-NLP/opus-mt-en-hi on cuda (fp16=True)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Reproducibility & backend\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if DEVICE == \"cuda\":\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "# Load model & tokenizer\n",
        "\n",
        "print(f\"Loading model: {MODEL_ID} on {DEVICE} (fp16={USE_FP16})\")\n",
        "tokenizer = MarianTokenizer.from_pretrained(MODEL_ID)\n",
        "model = MarianMTModel.from_pretrained(MODEL_ID)\n",
        "if USE_FP16:\n",
        "    model = model.half()\n",
        "model = model.to(DEVICE).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468,
          "referenced_widgets": [
            "f97038498d644ee08d88893bcb93283b",
            "768140d392754aaeabc1eb1988684d38",
            "a29acd796f6c4ebbb17f3b88187c52f9",
            "4471ba99d7bf4aa586892e3d95383e11",
            "dc4a7be89f6e4da8a01dbd5125b845a2",
            "81eb4cde62354d44b0a17480ae920bb3",
            "1bcc1c6dcd9649c08a44f603929fa85a",
            "511a5fd4c7cd4216a2858e2423f06357",
            "8b4b23dc2d6c428fb63434f1c2197a6b",
            "2245649086b742fa872af113ccc6ede6",
            "61f8952a0fc545d8933234175cd20d33"
          ]
        },
        "id": "rbHg46NUD50-",
        "outputId": "31b0d4ec-694e-44e5-8700-1e374924b44c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset: cfilt/iitb-english-hindi [test]\n",
            "\n",
            "=== First 5 examples from IITB English-Hindi (test) ===\n",
            "\n",
            "Example 1:\n",
            "SRC: A black box in your car?\n",
            "REF: आपकी कार में ब्लैक बॉक्स?\n",
            "\n",
            "Example 2:\n",
            "SRC: As America's road planners struggle to find the cash to mend a crumbling highway system, many are beginning to see a solution in a little black box that fits neatly by the dashboard of your car.\n",
            "REF: जबकि अमेरिका के सड़क योजनाकार, ध्वस्त होते हुए हाईवे सिस्टम को सुधारने के लिए धन की कमी से जूझ रहे हैं, वहीं बहुत-से लोग इसका समाधान छोटे से ब्लैक बॉक्स में देख रहे हैं, जो आपकी कार के डैशबोर्ड पर सफ़ाई से फिट हो जाता है।\n",
            "\n",
            "Example 3:\n",
            "SRC: The devices, which track every mile a motorist drives and transmit that information to bureaucrats, are at the center of a controversial attempt in Washington and state planning offices to overhaul the outdated system for funding America's major roads.\n",
            "REF: यह डिवाइस, जो मोटर-चालक द्वारा वाहन चलाए गए प्रत्येक मील को ट्रैक करती है तथा उस सूचना को अधिकारियों को संचारित करती है, आजकल अमेरिका की प्रमुख सड़कों का वित्त-पोषण करने के लिए पुराने हो चुके सिस्टम का जीर्णोद्धार करने के लिए वाशिंगटन और राज्य नियोजन कार्यालय के लिए एक विवादास्पद प्रयास का मुद्दा बन चुका है।\n",
            "\n",
            "Example 4:\n",
            "SRC: The usually dull arena of highway planning has suddenly spawned intense debate and colorful alliances.\n",
            "REF: आम तौर पर हाईवे नियोजन जैसा उबाऊ काम भी अचानक गहन बहस तथा जीवंत गठबंधनों का मुद्दा बन गया है।\n",
            "\n",
            "Example 5:\n",
            "SRC: Libertarians have joined environmental groups in lobbying to allow government to use the little boxes to keep track of the miles you drive, and possibly where you drive them - then use the information to draw up a tax bill.\n",
            "REF: आपने द्वारा ड्राइव किए गए मील, तथा संभवतः ड्राइव किए गए स्थान का विवरण रखने - और फिर इस सूचना का उपयोग टैक्स बिल तैयार करने के लिए - सरकार को इन ब्लैक बॉक्स का उपयोग करने की अनुमति देने के पक्ष में समर्थन जुटाने के लिए लिबरेटेरियन पर्यावरणीय समूहों के साथ मिल गए हैं।\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load dataset\n",
        "\n",
        "print(f\"Loading dataset: {DATASET_ID} [{SPLIT}]\")\n",
        "ds = load_dataset(DATASET_ID, split=SPLIT)\n",
        "\n",
        "def extract_pair(example):\n",
        "    tr = example[\"translation\"]\n",
        "    return {\"src\": tr[\"en\"].strip(), \"ref\": tr[\"hi\"].strip()}\n",
        "\n",
        "ds = ds.map(extract_pair, remove_columns=ds.column_names)\n",
        "\n",
        "# Show first 5 raw examples\n",
        "print(\"\\n=== First 5 examples from IITB English-Hindi (test) ===\")\n",
        "for i in range(min(5, len(ds))):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(\"SRC:\", ds[i][\"src\"])\n",
        "    print(\"REF:\", ds[i][\"ref\"])\n",
        "\n",
        "# Safety check (we want full test)\n",
        "if EVAL_SIZE is not None and EVAL_SIZE < len(ds):\n",
        "    raise ValueError(\"EVAL_SIZE must be None to evaluate the entire test dataset.\")\n",
        "\n",
        "srcs = ds[\"src\"]\n",
        "refs = ds[\"ref\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8wYIsi4D54D",
        "outputId": "3bebf836-b2f9-4ce0-de1d-baf27beba8b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Translating… (full test set)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/40 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacity of 3.68 GiB of which 49.81 MiB is free. Process 180147 has 2.54 GiB memory in use. Including non-PyTorch memory, this process has 1.06 GiB memory in use. Of the allocated memory 886.30 MiB is allocated by PyTorch, and 105.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(srcs), BATCH_SIZE)):\n\u001b[32m     22\u001b[39m     batch_src = srcs[i:i+BATCH_SIZE]\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     hyps.extend(\u001b[43mtranslate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_src\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(hyps) == \u001b[38;5;28mlen\u001b[39m(refs)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Light normalization (Hindi)\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m#is to clean and standardize Hindi text so that small, meaningless differences\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m#(like spacing or punctuation inconsistencies) don’t unfairly affect evaluation metrics such as BLEU\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mtranslate_batch\u001b[39m\u001b[34m(texts)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m enc:\n\u001b[32m      7\u001b[39m     enc[k] = enc[k].to(DEVICE)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m gen = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_NEW_TOKENS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_BEAMS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLENGTH_PENALTY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNO_REPEAT_NGRAM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m out = tokenizer.batch_decode(gen, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [o.strip() \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m out]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages/transformers/generation/utils.py:3282\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   3279\u001b[39m \u001b[38;5;66;03m# b. Compute log probs -- get log probabilities from logits, process logits with processors (*e.g.*\u001b[39;00m\n\u001b[32m   3280\u001b[39m \u001b[38;5;66;03m# `temperature`, ...), and add new logprobs to existing running logprobs scores.\u001b[39;00m\n\u001b[32m   3281\u001b[39m log_probs = nn.functional.log_softmax(logits, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3282\u001b[39m log_probs = \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_running_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3284\u001b[39m \u001b[38;5;66;03m# Store logits, attentions and hidden_states when required\u001b[39;00m\n\u001b[32m   3285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages/transformers/generation/logits_process.py:93\u001b[39m, in \u001b[36mLogitsProcessorList.__call__\u001b[39m\u001b[34m(self, input_ids, scores, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         scores = processor(input_ids, scores, **kwargs)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m         scores = \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/content/local/lib/python3.12/site-packages/transformers/generation/logits_process.py:1203\u001b[39m, in \u001b[36mSequenceBiasLogitsProcessor.__call__\u001b[39m\u001b[34m(self, input_ids, scores)\u001b[39m\n\u001b[32m   1196\u001b[39m     bias[:, last_token] += torch.where(\n\u001b[32m   1197\u001b[39m         matching_rows.bool(),\n\u001b[32m   1198\u001b[39m         torch.tensor(sequence_bias, device=input_ids.device),\n\u001b[32m   1199\u001b[39m         torch.tensor(\u001b[32m0.0\u001b[39m, device=input_ids.device),\n\u001b[32m   1200\u001b[39m     )\n\u001b[32m   1202\u001b[39m \u001b[38;5;66;03m# 5 - apply the bias to the scores\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1203\u001b[39m scores_processed = \u001b[43mscores\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m scores_processed\n",
            "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacity of 3.68 GiB of which 49.81 MiB is free. Process 180147 has 2.54 GiB memory in use. Including non-PyTorch memory, this process has 1.06 GiB memory in use. Of the allocated memory 886.30 MiB is allocated by PyTorch, and 105.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "\n",
        "# Translation (beam search)\n",
        "\n",
        "@torch.inference_mode()\n",
        "def translate_batch(texts):\n",
        "    enc = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    for k in enc:\n",
        "        enc[k] = enc[k].to(DEVICE)\n",
        "    gen = model.generate(\n",
        "        **enc,\n",
        "        max_new_tokens=MAX_NEW_TOKENS,\n",
        "        num_beams=NUM_BEAMS,\n",
        "        length_penalty=LENGTH_PENALTY,\n",
        "        no_repeat_ngram_size=NO_REPEAT_NGRAM,\n",
        "        use_cache=True,\n",
        "    )\n",
        "    out = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "    return [o.strip() for o in out]\n",
        "\n",
        "print(\"\\nTranslating… (full test set)\")\n",
        "hyps = []\n",
        "for i in tqdm(range(0, len(srcs), BATCH_SIZE)):\n",
        "    batch_src = srcs[i:i+BATCH_SIZE]\n",
        "    hyps.extend(translate_batch(batch_src))\n",
        "\n",
        "assert len(hyps) == len(refs)\n",
        "\n",
        "\n",
        "# Light normalization (Hindi)\n",
        "#is to clean and standardize Hindi text so that small, meaningless differences\n",
        "#(like spacing or punctuation inconsistencies) don’t unfairly affect evaluation metrics such as BLEU\n",
        "\n",
        "def normalize_hi(s: str) -> str:\n",
        "    s = s.strip()\n",
        "    # unify danda/full stop spacing and collapse spaces\n",
        "    s = s.replace(\" ।\", \"।\").replace(\" .\", \".\")\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "hyps = [normalize_hi(x) for x in hyps]\n",
        "refs = [normalize_hi(x) for x in refs]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-V5csVQ0EaSz",
        "outputId": "51e707fb-db75-4ed6-9586-ba73e8dc3c10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Computing corpus BLEU (sacrebleu)…\n",
            "\n",
            "=== Corpus BLEU (EN→HI, IITB full test) ===\n",
            "BLEU 13a : 10.00\n",
            "BLEU intl: 10.46  (recommended for Indic)\n",
            "\n",
            "Scoring sentences (sentence-level BLEU, intl)…\n",
            "\n",
            "=== Top 5 correct examples (by sentence BLEU, intl) ===\n",
            "\n",
            "[1] BLEU=78.25\n",
            "SRC: This project is a key element of energy security of the whole European continent.\n",
            "REF: यह परियोजना पूरे यूरोपीय महाद्वीप की ऊर्जा सुरक्षा का एक मुख्य तत्व है।\n",
            "HYP: यह परियोजना पूरे यूरोपीय महाद्वीप की ऊर्जा सुरक्षा का एक प्रमुख तत्व है।\n",
            "\n",
            "[2] BLEU=75.98\n",
            "SRC: What was the cause of death?\n",
            "REF: मौत का कारण क्या था?\n",
            "HYP: मृत्यु का कारण क्या था?\n",
            "\n",
            "[3] BLEU=74.48\n",
            "SRC: Someone in the German parliament says we should build a German Google.\n",
            "REF: जर्मन संसद में किसी ने कहा की हमे एक जर्मन गूगल का निर्माण करना चाहिए।\n",
            "HYP: जर्मन संसद में किसी ने कहा कि हम एक जर्मन गूगल का निर्माण करना चाहिए।\n",
            "\n",
            "[4] BLEU=70.17\n",
            "SRC: Frontier has gone the furthest in this area, though.\n",
            "REF: Frontier हालांकि, इस क्षेत्र में आगे चला गया है.\n",
            "HYP: हालांकि, इस क्षेत्र में सबसे आगे चला गया है.\n",
            "\n",
            "[5] BLEU=67.32\n",
            "SRC: What about the first?\n",
            "REF: पहले वाले के बारे में क्या?\n",
            "HYP: पहले के बारे में क्या?\n",
            "\n",
            "=== Bottom 5 bad-performing examples (by sentence BLEU, intl) ===\n",
            "\n",
            "[1] BLEU=0.00\n",
            "SRC: Wonks call it a mileage-based user fee.\n",
            "REF: वॉन्क्स इसे माइलेज-आधारित उपयोगकर्ता शुल्क कहते हैं।\n",
            "HYP: यह एक मील की दूरी पर उपयोक्ता कैश कॉल करता है.\n",
            "\n",
            "[2] BLEU=0.00\n",
            "SRC: Presenting CAT's trendy footwear and clothing collection.\n",
            "REF: कैट की ट्रैडी फुटवियर और परिधानों की सिलेक्शन पेश\n",
            "HYP: सी. टी. वी.\n",
            "\n",
            "[3] BLEU=0.00\n",
            "SRC: CAT is a brand that has been offering strong, durable, beautiful and high quality products for the past one hundred years.\n",
            "REF: कैट एक ऐसा ब्राड है जो पिछले सौ साल से मजबूत, टिकाऊ, सुंदर और बेहतरीन उत्पाद पेश कर रहा है।\n",
            "HYP: सी. टी. वी.\n",
            "\n",
            "[4] BLEU=0.00\n",
            "SRC: CAT, in a way, is the blend of durability and lifestyle.\n",
            "REF: कैट एक तरह से मजबूती और लाइफ स्टाइल का मिलन है।\n",
            "HYP: सी. टी.\n",
            "\n",
            "[5] BLEU=0.00\n",
            "SRC: MRF has been awarded the JD Power Award, for the tenth time in a row.\n",
            "REF: एमआरएफ लगातार दसवीं बार जेडी पावर पुरस्कार से सम्मानित\n",
            "HYP: MRF को JD शक्‍ति Aagad, एक पंक्ति में दसवें समय के लिए दिया गया है.\n",
            "\n",
            "Saved to: eval_outputs_en_hi_iitb_full_bleu/ (sources.txt, references.txt, hypotheses.txt)\n",
            "\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Corpus BLEU (13a and intl)\n",
        "\n",
        "print(\"\\nComputing corpus BLEU (sacrebleu)…\")\n",
        "bleu_13a  = sacrebleu.corpus_bleu(hyps, [refs], tokenize=\"13a\")\n",
        "bleu_intl = sacrebleu.corpus_bleu(hyps, [refs], tokenize=\"intl\")\n",
        "\n",
        "print(\"\\n=== Corpus BLEU (EN→HI, IITB full test) ===\")\n",
        "print(f\"BLEU 13a : {bleu_13a.score:.2f}\")\n",
        "print(f\"BLEU intl: {bleu_intl.score:.2f}  (recommended for Indic)\")\n",
        "\n",
        "\n",
        "# Sentence-level ranking (BLEU intl)\n",
        "\n",
        "print(\"\\nScoring sentences (sentence-level BLEU, intl)…\")\n",
        "sent_bleu_scores = [\n",
        "    sacrebleu.sentence_bleu(h, [r], tokenize=\"intl\").score\n",
        "    for h, r in zip(hyps, refs)\n",
        "]\n",
        "\n",
        "def topk_indices(scores, k, reverse=True):\n",
        "    return sorted(range(len(scores)), key=lambda i: scores[i], reverse=reverse)[:k]\n",
        "\n",
        "K = 5\n",
        "best_idx = topk_indices(sent_bleu_scores, K, reverse=True)\n",
        "worst_idx = topk_indices(sent_bleu_scores, K, reverse=False)\n",
        "\n",
        "def pretty_show(indices, title):\n",
        "    print(f\"\\n=== {title} (by sentence BLEU, intl) ===\")\n",
        "    for rank, i in enumerate(indices, 1):\n",
        "        print(f\"\\n[{rank}] BLEU={sent_bleu_scores[i]:.2f}\")\n",
        "        print(f\"SRC: {srcs[i]}\")\n",
        "        print(f\"REF: {refs[i]}\")\n",
        "        print(f\"HYP: {hyps[i]}\")\n",
        "\n",
        "pretty_show(best_idx, \"Top 5 correct examples\")\n",
        "pretty_show(worst_idx, \"Bottom 5 bad-performing examples\")\n",
        "\n",
        "\n",
        "# Save outputs (optional)\n",
        "\n",
        "out_dir = \"eval_outputs_en_hi_iitb_full_bleu\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "with open(os.path.join(out_dir, \"sources.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(srcs))\n",
        "with open(os.path.join(out_dir, \"references.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(refs))\n",
        "with open(os.path.join(out_dir, \"hypotheses.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(hyps))\n",
        "\n",
        "print(f\"\\nSaved to: {out_dir}/ (sources.txt, references.txt, hypotheses.txt)\")\n",
        "print(\"\\nDone.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "36b77e1f1065449693df8a2d2ef97f45",
            "6d8a042c7697466cb2d7a1d9482b433f",
            "2e5d1fc13fbe444989ff6ca3fdc896f9",
            "7ed71dc683d14a9e872aa121edc8e718",
            "96596f31a6f14d2599e593e56f678246",
            "1f67dca72dac45709a592b3e1bfb3b65",
            "a29ef53b164a49a6bb738e4778c21682",
            "a8a2adc36d564db9ba95b57ff18ff954",
            "2e7e7f4dd6a9469cbb969e61512f237f",
            "1bf4772725e04766bbea8059c486604a",
            "49d5cbf448d34a17bced95439ed0a647"
          ]
        },
        "id": "SIJxkkyaBoBy",
        "outputId": "3e70d3ad-cc5b-4fcb-ffd2-c76bcd222c5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: Helsinki-NLP/opus-mt-en-hi on cuda (fp16=True)\n",
            "Loading dataset: cfilt/iitb-english-hindi [test]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36b77e1f1065449693df8a2d2ef97f45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2507 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== First 5 examples from IITB English-Hindi (test) ===\n",
            "\n",
            "Example 1:\n",
            "SRC: A black box in your car?\n",
            "REF: आपकी कार में ब्लैक बॉक्स?\n",
            "\n",
            "Example 2:\n",
            "SRC: As America's road planners struggle to find the cash to mend a crumbling highway system, many are beginning to see a solution in a little black box that fits neatly by the dashboard of your car.\n",
            "REF: जबकि अमेरिका के सड़क योजनाकार, ध्वस्त होते हुए हाईवे सिस्टम को सुधारने के लिए धन की कमी से जूझ रहे हैं, वहीं बहुत-से लोग इसका समाधान छोटे से ब्लैक बॉक्स में देख रहे हैं, जो आपकी कार के डैशबोर्ड पर सफ़ाई से फिट हो जाता है।\n",
            "\n",
            "Example 3:\n",
            "SRC: The devices, which track every mile a motorist drives and transmit that information to bureaucrats, are at the center of a controversial attempt in Washington and state planning offices to overhaul the outdated system for funding America's major roads.\n",
            "REF: यह डिवाइस, जो मोटर-चालक द्वारा वाहन चलाए गए प्रत्येक मील को ट्रैक करती है तथा उस सूचना को अधिकारियों को संचारित करती है, आजकल अमेरिका की प्रमुख सड़कों का वित्त-पोषण करने के लिए पुराने हो चुके सिस्टम का जीर्णोद्धार करने के लिए वाशिंगटन और राज्य नियोजन कार्यालय के लिए एक विवादास्पद प्रयास का मुद्दा बन चुका है।\n",
            "\n",
            "Example 4:\n",
            "SRC: The usually dull arena of highway planning has suddenly spawned intense debate and colorful alliances.\n",
            "REF: आम तौर पर हाईवे नियोजन जैसा उबाऊ काम भी अचानक गहन बहस तथा जीवंत गठबंधनों का मुद्दा बन गया है।\n",
            "\n",
            "Example 5:\n",
            "SRC: Libertarians have joined environmental groups in lobbying to allow government to use the little boxes to keep track of the miles you drive, and possibly where you drive them - then use the information to draw up a tax bill.\n",
            "REF: आपने द्वारा ड्राइव किए गए मील, तथा संभवतः ड्राइव किए गए स्थान का विवरण रखने - और फिर इस सूचना का उपयोग टैक्स बिल तैयार करने के लिए - सरकार को इन ब्लैक बॉक्स का उपयोग करने की अनुमति देने के पक्ष में समर्थन जुटाने के लिए लिबरेटेरियन पर्यावरणीय समूहों के साथ मिल गए हैं।\n",
            "\n",
            "Translating… (full test set)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [00:24<00:00,  1.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Computing corpus BLEU (sacrebleu)…\n",
            "\n",
            "=== Corpus BLEU (EN→HI, IITB full test) ===\n",
            "BLEU: 8.96\n",
            "\n",
            "Scoring sentences (sentence-level BLEU)…\n",
            "\n",
            "=== Top 5 correct examples (by sentence BLEU) ===\n",
            "\n",
            "[1] BLEU=100.00\n",
            "SRC: What was the cause of death?\n",
            "REF: मौत का कारण क्या था?\n",
            "HYP: मौत का कारण क्या था?\n",
            "\n",
            "[2] BLEU=69.31\n",
            "SRC: This project is a key element of energy security of the whole European continent.\n",
            "REF: यह परियोजना पूरे यूरोपीय महाद्वीप की ऊर्जा सुरक्षा का एक मुख्य तत्व है।\n",
            "HYP: यह परियोजना पूरे यूरोपीय महाद्वीप की ऊर्जा सुरक्षा का एक प्रमुख तत्व है ।\n",
            "\n",
            "[3] BLEU=67.32\n",
            "SRC: What about the first?\n",
            "REF: पहले वाले के बारे में क्या?\n",
            "HYP: पहले के बारे में क्या?\n",
            "\n",
            "[4] BLEU=63.89\n",
            "SRC: And I think about my father.\n",
            "REF: और मैं अपने पिता के बारे में सोचता हूँ।\n",
            "HYP: और मैं अपने पिता के बारे में सोचते हैं.\n",
            "\n",
            "[5] BLEU=60.04\n",
            "SRC: Share with us your thoughts in the comments below.\n",
            "REF: अपने विचार नीचे टिप्पणियों में हमारे साथ साझा करें।\n",
            "HYP: नीचे टिप्पणियों में हमारे साथ साझा करें.\n",
            "\n",
            "=== Bottom 5 bad-performing examples (by sentence BLEU) ===\n",
            "\n",
            "[1] BLEU=0.00\n",
            "SRC: Wonks call it a mileage-based user fee.\n",
            "REF: वॉन्क्स इसे माइलेज-आधारित उपयोगकर्ता शुल्क कहते हैं।\n",
            "HYP: यह एक मील की दूरी पर उपयोक्ता भुगतान कॉल करता है.\n",
            "\n",
            "[2] BLEU=0.00\n",
            "SRC: In Oregon, planners are experimenting with giving drivers different choices.\n",
            "REF: ऑरेगॉन में, योजनाकार चालकों को भिन्न-भिन्न विकल्प देने का प्रयोग कर रहे हैं।\n",
            "HYP: एक और मिसाल लीजिए ।\n",
            "\n",
            "[3] BLEU=0.00\n",
            "SRC: The Maharaja of the dynastic capital has arranged some magnificent cuisine from the majestic palaces.\n",
            "REF: खानदानी राजधानी के महाराज ने आपके लिए राजसी महलों के शानदार व्यंजनों का प्रबंध किया है।\n",
            "HYP: इस द्वीप की शान बहुत ही खूबसूरत थी ।\n",
            "\n",
            "[4] BLEU=0.00\n",
            "SRC: CAT is a brand that has been offering strong, durable, beautiful and high quality products for the past one hundred years.\n",
            "REF: कैट एक ऐसा ब्राड है जो पिछले सौ साल से मजबूत, टिकाऊ, सुंदर और बेहतरीन उत्पाद पेश कर रहा है।\n",
            "HYP: सी. टी.\n",
            "\n",
            "[5] BLEU=0.00\n",
            "SRC: CAT, in a way, is the blend of durability and lifestyle.\n",
            "REF: कैट एक तरह से मजबूती और लाइफ स्टाइल का मिलन है।\n",
            "HYP: सी. टी.\n",
            "\n",
            "Saved to: eval_outputs_en_hi_iitb_full_bleu/ (sources.txt, references.txt, hypotheses.txt)\n",
            "\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Fast IITB EN→HI evaluation with Helsinki-NLP/opus-mt-en-hi\n",
        "- Prints first 5 dataset examples\n",
        "- FULL test evaluation (no subsetting)\n",
        "- BLEU via sacrebleu (corpus + sentence-level)\n",
        "- 5 best / 5 worst examples by sentence BLEU\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import sacrebleu\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Config (tune these for speed/quality)\n",
        "\n",
        "MODEL_ID = \"Helsinki-NLP/opus-mt-en-hi\"\n",
        "DATASET_ID = \"cfilt/iitb-english-hindi\"\n",
        "SPLIT = \"test\"\n",
        "\n",
        "# Evaluate FULL test set\n",
        "EVAL_SIZE = None          # <-- full test; do not change unless you want a subset\n",
        "BATCH_SIZE = 64           # adjust for your hardware\n",
        "MAX_NEW_TOKENS = 96\n",
        "NUM_BEAMS = 1             # 1 = greedy (keep same behavior)\n",
        "NO_REPEAT_NGRAM = 0\n",
        "SEED = 42\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "USE_FP16 = DEVICE == \"cuda\"  # enable half-precision on GPU\n",
        "\n",
        "\n",
        "# Reproducibility & backend\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if DEVICE == \"cuda\":\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "# Load model & tokenizer\n",
        "\n",
        "print(f\"Loading model: {MODEL_ID} on {DEVICE} (fp16={USE_FP16})\")\n",
        "tokenizer = MarianTokenizer.from_pretrained(MODEL_ID)\n",
        "model = MarianMTModel.from_pretrained(MODEL_ID)\n",
        "if USE_FP16:\n",
        "    model = model.half()\n",
        "model = model.to(DEVICE).eval()\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "\n",
        "print(f\"Loading dataset: {DATASET_ID} [{SPLIT}]\")\n",
        "ds = load_dataset(DATASET_ID, split=SPLIT)\n",
        "\n",
        "def extract_pair(example):\n",
        "    tr = example[\"translation\"]\n",
        "    return {\"src\": tr[\"en\"].strip(), \"ref\": tr[\"hi\"].strip()}\n",
        "\n",
        "ds = ds.map(extract_pair, remove_columns=ds.column_names)\n",
        "\n",
        "# Show first 5 raw examples\n",
        "print(\"\\n=== First 5 examples from IITB English-Hindi (test) ===\")\n",
        "for i in range(5):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(\"SRC:\", ds[i][\"src\"])\n",
        "    print(\"REF:\", ds[i][\"ref\"])\n",
        "\n",
        "# (No subsetting; evaluate full test set)\n",
        "if EVAL_SIZE is not None and EVAL_SIZE < len(ds):\n",
        "    raise ValueError(\"EVAL_SIZE is not None; set it to None to evaluate the entire test dataset.\")\n",
        "\n",
        "srcs = ds[\"src\"]\n",
        "refs = ds[\"ref\"]\n",
        "\n",
        "# Batched translation\n",
        "\n",
        "@torch.inference_mode()\n",
        "def translate_batch(texts):\n",
        "    enc = tokenizer(\n",
        "        texts, return_tensors=\"pt\", padding=True, truncation=True\n",
        "    )\n",
        "    # Move to device\n",
        "    for k in enc:\n",
        "        enc[k] = enc[k].to(DEVICE)\n",
        "    gen = model.generate(\n",
        "        **enc,\n",
        "        max_new_tokens=MAX_NEW_TOKENS,\n",
        "        num_beams=NUM_BEAMS,\n",
        "        length_penalty=1.0,\n",
        "        no_repeat_ngram_size=NO_REPEAT_NGRAM,\n",
        "        use_cache=True,\n",
        "    )\n",
        "    out = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "    return [o.strip() for o in out]\n",
        "\n",
        "print(\"\\nTranslating… (full test set)\")\n",
        "hyps = []\n",
        "for i in tqdm(range(0, len(srcs), BATCH_SIZE)):\n",
        "    batch_src = srcs[i:i+BATCH_SIZE]\n",
        "    hyps.extend(translate_batch(batch_src))\n",
        "\n",
        "assert len(hyps) == len(refs)\n",
        "\n",
        "\n",
        "# Corpus BLEU only\n",
        "\n",
        "print(\"\\nComputing corpus BLEU (sacrebleu)…\")\n",
        "bleu = sacrebleu.corpus_bleu(hyps, [refs])\n",
        "\n",
        "print(\"\\n=== Corpus BLEU (EN→HI, IITB full test) ===\")\n",
        "print(f\"BLEU: {bleu.score:.2f}\")\n",
        "\n",
        "\n",
        "# Sentence-level ranking (BLEU)\n",
        "\n",
        "print(\"\\nScoring sentences (sentence-level BLEU)…\")\n",
        "# sacrebleu.sentence_bleu returns an object with .score\n",
        "sent_bleu_scores = [sacrebleu.sentence_bleu(h, [r]).score for h, r in zip(hyps, refs)]\n",
        "\n",
        "def topk_indices(scores, k, reverse=True):\n",
        "    return sorted(range(len(scores)), key=lambda i: scores[i], reverse=reverse)[:k]\n",
        "\n",
        "K = 5\n",
        "best_idx = topk_indices(sent_bleu_scores, K, reverse=True)\n",
        "worst_idx = topk_indices(sent_bleu_scores, K, reverse=False)\n",
        "\n",
        "def pretty_show(indices, title):\n",
        "    print(f\"\\n=== {title} (by sentence BLEU) ===\")\n",
        "    for rank, i in enumerate(indices, 1):\n",
        "        print(f\"\\n[{rank}] BLEU={sent_bleu_scores[i]:.2f}\")\n",
        "        print(f\"SRC: {srcs[i]}\")\n",
        "        print(f\"REF: {refs[i]}\")\n",
        "        print(f\"HYP: {hyps[i]}\")\n",
        "\n",
        "pretty_show(best_idx, \"Top 5 correct examples\")\n",
        "pretty_show(worst_idx, \"Bottom 5 bad-performing examples\")\n",
        "\n",
        "\n",
        "# Save outputs (optional)\n",
        "\n",
        "out_dir = \"eval_outputs_en_hi_iitb_full_bleu\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "with open(os.path.join(out_dir, \"sources.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(srcs))\n",
        "with open(os.path.join(out_dir, \"references.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(refs))\n",
        "with open(os.path.join(out_dir, \"hypotheses.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(hyps))\n",
        "\n",
        "print(f\"\\nSaved to: {out_dir}/ (sources.txt, references.txt, hypotheses.txt)\")\n",
        "print(\"\\nDone.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1bcc1c6dcd9649c08a44f603929fa85a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bf4772725e04766bbea8059c486604a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f67dca72dac45709a592b3e1bfb3b65": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2245649086b742fa872af113ccc6ede6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e5d1fc13fbe444989ff6ca3fdc896f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8a2adc36d564db9ba95b57ff18ff954",
            "max": 2507,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e7e7f4dd6a9469cbb969e61512f237f",
            "value": 2507
          }
        },
        "2e7e7f4dd6a9469cbb969e61512f237f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36b77e1f1065449693df8a2d2ef97f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d8a042c7697466cb2d7a1d9482b433f",
              "IPY_MODEL_2e5d1fc13fbe444989ff6ca3fdc896f9",
              "IPY_MODEL_7ed71dc683d14a9e872aa121edc8e718"
            ],
            "layout": "IPY_MODEL_96596f31a6f14d2599e593e56f678246"
          }
        },
        "4471ba99d7bf4aa586892e3d95383e11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2245649086b742fa872af113ccc6ede6",
            "placeholder": "​",
            "style": "IPY_MODEL_61f8952a0fc545d8933234175cd20d33",
            "value": " 2507/2507 [00:00&lt;00:00, 19198.26 examples/s]"
          }
        },
        "49d5cbf448d34a17bced95439ed0a647": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "511a5fd4c7cd4216a2858e2423f06357": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61f8952a0fc545d8933234175cd20d33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d8a042c7697466cb2d7a1d9482b433f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f67dca72dac45709a592b3e1bfb3b65",
            "placeholder": "​",
            "style": "IPY_MODEL_a29ef53b164a49a6bb738e4778c21682",
            "value": "Map: 100%"
          }
        },
        "768140d392754aaeabc1eb1988684d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81eb4cde62354d44b0a17480ae920bb3",
            "placeholder": "​",
            "style": "IPY_MODEL_1bcc1c6dcd9649c08a44f603929fa85a",
            "value": "Map: 100%"
          }
        },
        "7ed71dc683d14a9e872aa121edc8e718": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bf4772725e04766bbea8059c486604a",
            "placeholder": "​",
            "style": "IPY_MODEL_49d5cbf448d34a17bced95439ed0a647",
            "value": " 2507/2507 [00:00&lt;00:00, 19647.78 examples/s]"
          }
        },
        "81eb4cde62354d44b0a17480ae920bb3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b4b23dc2d6c428fb63434f1c2197a6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96596f31a6f14d2599e593e56f678246": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a29acd796f6c4ebbb17f3b88187c52f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_511a5fd4c7cd4216a2858e2423f06357",
            "max": 2507,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b4b23dc2d6c428fb63434f1c2197a6b",
            "value": 2507
          }
        },
        "a29ef53b164a49a6bb738e4778c21682": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8a2adc36d564db9ba95b57ff18ff954": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc4a7be89f6e4da8a01dbd5125b845a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f97038498d644ee08d88893bcb93283b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_768140d392754aaeabc1eb1988684d38",
              "IPY_MODEL_a29acd796f6c4ebbb17f3b88187c52f9",
              "IPY_MODEL_4471ba99d7bf4aa586892e3d95383e11"
            ],
            "layout": "IPY_MODEL_dc4a7be89f6e4da8a01dbd5125b845a2"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
