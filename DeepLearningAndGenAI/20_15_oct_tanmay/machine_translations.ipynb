{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccmjbDMuA_q7",
        "outputId": "bbd5fc17-5349-4097-94e9-1f136cf7eb16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: SentencePiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from sacremoses) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from sacremoses) (1.5.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: sacremoses, portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.2.0 sacrebleu-2.5.1 sacremoses-0.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sacrebleu datasets transformers SentencePiece sacremoses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q85mhytyMfS"
      },
      "source": [
        "BLEU measures n-gram overlap between:\n",
        "\n",
        "The machine-generated translation (called hypothesis), and\n",
        "\n",
        "The reference translation(s) (human-annotated ground truth).\n",
        "\n",
        "It checks how many words, word pairs, triplets, etc. match between them.\n",
        "\n",
        "Reference:  The cat is on the mat\n",
        "Hypothesis: The cat sat on the mat\n",
        "\n",
        "Unigrams (n=1):\n",
        "\n",
        "Overlap words: “The”, “cat”, “on”, “the”, “mat” → 5 matches out of 6 in hypothesis\n",
        "\n",
        "Bigrams (n=2):\n",
        "\n",
        "Reference: \"The cat\", \"cat is\", \"is on\", \"on the\", \"the mat\"\n",
        "\n",
        "Hypothesis: \"The cat\", \"cat sat\", \"sat on\", \"on the\", \"the mat\"\n",
        "\n",
        "Overlaps: \"The cat\", \"on the\", \"the mat\" → 3 matches out of 5\n",
        "\n",
        "Apply brevity penalty (BP)\n",
        "\n",
        "If the hypothesis is too short, it gets penalized.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "78czNQihDP5f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import sacrebleu\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Config\n",
        "\n",
        "MODEL_ID = \"Helsinki-NLP/opus-mt-en-hi\"\n",
        "DATASET_ID = \"cfilt/iitb-english-hindi\"\n",
        "SPLIT = \"test\"\n",
        "\n",
        "# Full test evaluation\n",
        "EVAL_SIZE = None\n",
        "BATCH_SIZE = 64\n",
        "MAX_NEW_TOKENS = 128\n",
        "NUM_BEAMS = 6\n",
        "LENGTH_PENALTY = 1.1     #Adjusts how much the model is penalized for producing long sequences in beam search.\n",
        "NO_REPEAT_NGRAM = 2   #Prevents the model from repeating the same 3-word sequence during generation\n",
        "SEED = 42\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "USE_FP16 = DEVICE == \"cuda\"   # half-precision on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415,
          "referenced_widgets": [
            "b341c1ea77634a9789d04542eca2d3b9",
            "58aba847f68643888eb5027405399220",
            "52dc9fe5dcde4b86ad07200dc1d58fba",
            "3f19dc0faceb40a8ae61039c5f9e54da",
            "c24ed0c4256b4a3a9c807789f8616ac4",
            "c8032f4f3f544b38a4cf036d9d4eaca7",
            "95761841a0ab40c691e1fbfcb3f05849",
            "14ceeda7f3dd4205a90a430fdbffa49c",
            "17ee869b760549f1a1f654124c1462c4",
            "b9615f0970034f28a0e448170195bd34",
            "3ca40851a9a1483c8aae728551fc1508",
            "0768ad32991a4f7cbbb9e3595e000284",
            "57854674ae83430bbf8e883d8237c81a",
            "0d917e8c0e044affab9c08a10cf137e2",
            "a76fabe0933d4721a9b7058598891d24",
            "0d2600325d53497e95d6108812894529",
            "99d90fe260244f6a942424a6d0395b73",
            "867f4db222234b1da10f6af8c9c04e58",
            "ca3b9fef268845a4b25e3e5074349449",
            "4659c2912fdf48bab007ff365810c1e8",
            "d0513fb1289342c495ad73aabef9f34f",
            "6392bc758077452b9abdef481f99a117",
            "a0803a1892f947b0911b45b641e27be6",
            "6b43a1f9831b4ccebff5ed60a9bf2601",
            "ec26bb6dbbf14e78b43f74a13076a20a",
            "e6bbcf3ad7dc4bcfbceae23e6251c9a8",
            "07767c4196a84bf98705c9c7195e4edf",
            "5972b1c7ca554b5985d4646c65c34d40",
            "9a19c83ae5e54881aefeb943947468b0",
            "784d0d60f8bb43a79908cb3a9d293065",
            "3fbfccaac6c045dd84db3b5eb7acb1d6",
            "442b274cebfb4794a76a13ff6a0be98c",
            "1465626148814b58aad6fcd9a2e1342b",
            "e3bd23fcd1834dac9a002704cfe78e22",
            "6cd29d3bf7ca4195abb3341a74eaa1d3",
            "50204360df5044018135c2c28fd1f946",
            "806396d7f8ab41938d0b8b5876f9ccf9",
            "b42b986b76bb43ccb18a8e0b63a94963",
            "c9b12562d127484da2937e8031800cd5",
            "93f0c6e4c32a4cc7a9f551f0d687b466",
            "d524987e51164f8ab1ea851e7ba63398",
            "42bb06ec5a334327a42cc08b6b17c06c",
            "d1bb90d51e4f4844a052b0fe1b288e84",
            "a902571ff58c4ffa99a6e9208b18f3b8",
            "e6315f7f2fa748f0b2120fbf7035676a",
            "9215e6c1aad2472e995e1ceec3e9ac51",
            "34fe7174e28a4e598942e00a1ef48031",
            "e3c703423c024b5da1271f4c31da7927",
            "d5a3684177d04242971e4ada07f8d83d",
            "06db676461ea44e6a37f7728e14085d5",
            "56bf82f98fc2449db7cd9447a43a56fc",
            "5fd0cacc65d74e13be3b6322f30caa58",
            "f169a01c7e354bc39b6d9c8c8e44dfe3",
            "6d48c8d256ba40d99402d82ab1b8639c",
            "0e1f7b5d4a6e4ab386cc6bb813a5e35d",
            "5a535e38b999418d8c2ba4621ee6e2d1",
            "66d6e7a510c7469093ad184f170f760c",
            "d23e6d8edbcf4216aebee81245e756db",
            "2c409a3ddfbd43b3a68f8546f04e8084",
            "8a2421fac5364c088e5e07e53beb301d",
            "89696962d44342d6b67d438614dd6b8d",
            "6946ac31fef2462fb06529d8192396b2",
            "981dc304d03f414eada89c301f0b7e11",
            "016c14efd9a64988a4eafd8789ecb2a8",
            "f996b18920564a229cc93a108a03f34f",
            "b026d730487a49cabe890f16e04a928b",
            "23cc8f6c1b9e4f66b5fb5ce204b260a6",
            "3b26f56ebbaf44249f1d05eb14d824fa",
            "e5b0bc4dc9564e33818af60f1fd8782b",
            "0c17e1a1598e45c28965941c5576be90",
            "518625256d9b420188114f98e47bfaae",
            "889b0f932d1840cd9f4616e68718c583",
            "6552bf1b3bb341bf8feabc21e9426b3f",
            "20c3832385f24c3687cafff11e82bfaa",
            "842c34c0d7d74234bc45cdfcd0b72de1",
            "987aec38ff1e4bbf8a2dcc1126d89dc4",
            "07d9b44caf4a400abf83924541354016",
            "a7c291e509974d919d50c0a95a76b85e",
            "4dc598fbb5de4c32965fb3b758cd72de",
            "cc6082825ce1460eac9ce6c8b445cc31",
            "1a98fd36d367450389f7a81b107f068d",
            "2152a1d9e6bf4d6682f36bf33c39fafb",
            "94825e394d4d48299dd9b19ccee8c9ab",
            "ddcfa5b18b2649eab3b78983ec79763e",
            "b1645d7afcb34f2a8aa55799ee1c18c0",
            "b23b80c16f8c4ba192bb7e430a7801c8",
            "8b42c2413631412d9f01e0ce0a683e45",
            "c032926146d245a3b24dd2fbc385e15f"
          ]
        },
        "id": "pQaRqy87D5yD",
        "outputId": "74c6b526-fe8f-4bf3-fd7f-7033e0aebf44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: Helsinki-NLP/opus-mt-en-hi on cuda (fp16=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b341c1ea77634a9789d04542eca2d3b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "source.spm:   0%|          | 0.00/812k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0768ad32991a4f7cbbb9e3595e000284"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "target.spm:   0%|          | 0.00/1.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0803a1892f947b0911b45b641e27be6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3bd23fcd1834dac9a002704cfe78e22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6315f7f2fa748f0b2120fbf7035676a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/306M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a535e38b999418d8c2ba4621ee6e2d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23cc8f6c1b9e4f66b5fb5ce204b260a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/306M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7c291e509974d919d50c0a95a76b85e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "# Reproducibility & backend\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if DEVICE == \"cuda\":\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "# Load model & tokenizer\n",
        "\n",
        "print(f\"Loading model: {MODEL_ID} on {DEVICE} (fp16={USE_FP16})\")\n",
        "tokenizer = MarianTokenizer.from_pretrained(MODEL_ID)\n",
        "model = MarianMTModel.from_pretrained(MODEL_ID)\n",
        "if USE_FP16:\n",
        "    model = model.half()\n",
        "model = model.to(DEVICE).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724,
          "referenced_widgets": [
            "d32bd18b0ba94948837865677b72b9ff",
            "db7e921381c941ce94ac485067b29d76",
            "d1821eff25af49a38029f62d6a92092a",
            "92be03795de744588a536c76969d6289",
            "21f1c52e4a744a868b0f72ad91373857",
            "88b1eeb1ea1749dbb4a4f0fb01d99803",
            "c8313295540e45f18ba51f4313477577",
            "6480a9bb47f646b9bb6d170a88ee17c1",
            "fc6c033ef54c4783bd533e3d838a5092",
            "93da5005d4ec49c4930eaa8c67ca4968",
            "e15dfb17116e4169ae9bdfb3afa4df27",
            "fab60a427584453f90918c0d521c5709",
            "7cd98678565948818d0902d87feb8075",
            "95c0bab57e6244e2a828806ef4047988",
            "53bf0751314d46ca80dc5674f7852f66",
            "1ad5061fa9214a248436d250528010d8",
            "014c46406ad34651bcd99b39fab811af",
            "a65198c5acbb4c63b0a43bab47a06d61",
            "f634841265064f4bb78c88ebd5f960ed",
            "11bf5d5729f14c6daf7e385e49194492",
            "3314064321be4a159a1b1aee5e06094e",
            "0ccfd76dc2fb42a6bf7d76d65959c9b2",
            "e56a77c816da41269c74bfa547285827",
            "331a6a7da73f46248a21c61bce0cbc57",
            "3f8a86752ed94af6ab1fcf2f16f97494",
            "6d5018dacfda4a9b8281e7da7b5f5eb4",
            "6a08e563925c4deabdc0ddd60476d474",
            "9d2d5a01de594d0d8e316606848a0524",
            "308df94fe9404798baeb0cf9b2ebf2c1",
            "9200287d9f4c4ee9a9ca9de0cba009d0",
            "4e7c80c6241f401ca45ad83f5501c7e3",
            "d9ff3b5e47c34ade9b1674bd1ef1e122",
            "9e0a1acceaf54b31a3c1ba8da0a41ec6",
            "1d67c3f204b6471ca43848ed890ed7ba",
            "862f6ea7c47348e69abcfe96d1692847",
            "9e8f1261a72d47e9bcbc948a2c2c0d7d",
            "4a50acba71304496bb6fce909df805e7",
            "f72e0a69b06348538e93479546155e78",
            "0999b231284549439f5eeebbf1b54b00",
            "a625c9bfb9c94893944fcde020fb90d5",
            "348ba1b3ddb046dabb36c37c8e7bbfb3",
            "df513857674d4b4f9e56ce5bc75fe0b1",
            "7771686a09764b5197704a3732d6760a",
            "9e904105dd074a25b2e24d9915a8ab3f",
            "a70612137d0343e4acbfeeb416689cf0",
            "b2d7480b8b8440ceb8149afd34b15afd",
            "424b3e0023e54d50b30dd6b6ded8c926",
            "c9fecbf788d54d4f81d74b3cf648d151",
            "ba2e4ebe6f9c4a44a9fc175629f199db",
            "25a16422ae9a4ea5808501466d88abda",
            "d8f1f452e334475dbc2f3c75ed32ed2b",
            "5cc1634ec59f4360a771b897bd22684a",
            "7993c5417e7b4afe8d9a002cbdb48702",
            "eea01d51b4a047f4a811ac8aae247dc8",
            "b5d24412dff94a059f39961a9a157234",
            "340d8b846dbd4a29ae64fd8704af0872",
            "edadd003f330456d8f5604b640e6fae6",
            "b3f072cae6474a888af459c9f9bd28be",
            "b0b26f4cc23e435f899b3feb9a7e603c",
            "fc75e28e4ae3416e8be5043292c56800",
            "6009a99bfa9c4ee7af126027fdfe29e7",
            "610ab9b8aeaf42348dfc01fe9a8da07c",
            "88566a09d1cc41ff922042ff982c6a0e",
            "9ebad52118654e4483452f53ffb34fbe",
            "6a77118564e34e97a08e9752606a2113",
            "3602fdbb92184927be950ffd62b3e37d",
            "fa73af0a02dc42f0b0a8a4f559fec3df",
            "116eda254ebd4122ae19eec4462c8dcf",
            "2bc505ab80f74c7d8b70effdb54c4efb",
            "a9fad67c99e94460ad512b1b9cbec8ef",
            "0bc5c70e322942c7b0651071a6d49547",
            "92f89043be864e7e98cbe05eda83e83a",
            "9b385098b1494363a462ec497e60d3eb",
            "13d09c810bb649faa479d4b1f44dd5bb",
            "cbe2c2221cf7460ab31bf2f1a2e24a76",
            "df4b121a6c9446678dd598e8b9ce5326",
            "d7da408ea0ca4dd3b07924c59492bfc4",
            "f4c2bfa2a4cc4ed681c9130d8df31d60",
            "98ef663f55c84fe6806db6dad3c9fbe4",
            "702475aff7314bf4951b0455d2777b99",
            "e7ef477af2bd49b4ab69637d6d3d5cf1",
            "5027db34d8424476aa1a650397e8cbbd",
            "1f350d7ba0814005852c13a94d1e2d76",
            "4da0b09f626f42f9b9760fbdcc7a0c78",
            "7fb5bc046725412b909e80906110a282",
            "bb58c7bb22e2486d9a1a806c46b07e37",
            "7e52b9da82d049a7b38d97762ab45e15",
            "a0e67fc1972b44ad8033e7e7ffea0d79",
            "db539c7ef57243029ae0572b03c52373",
            "be8c2152e6f54d8aad934605951b6c44",
            "40429138e63145b184ed05310bedbfb6",
            "a3c975700c054d53b086fa0bfef6b620",
            "3859864db2c34be69653940117c3432c",
            "94f11d69c7d54c4da9bec3b965f1becf",
            "ab86e862c8654fea8fda5c1bdb23b43e",
            "0889341bcbcd4a0ab34223a1a879fc0e",
            "b0d525fbcb924311859c7449fb161d43",
            "a3ab1c8d942a4f70be1a7fa8a1ef5f48",
            "ee5db5381b144ce0971443fa81563f05"
          ]
        },
        "id": "rbHg46NUD50-",
        "outputId": "a0077ae7-64b4-43ee-b129-bbd3b27bb881"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset: cfilt/iitb-english-hindi [test]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d32bd18b0ba94948837865677b72b9ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "dataset_infos.json:   0%|          | 0.00/953 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fab60a427584453f90918c0d521c5709"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00000-of-00001.parquet:   0%|          | 0.00/190M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e56a77c816da41269c74bfa547285827"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/validation-00000-of-00001.parquet:   0%|          | 0.00/85.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d67c3f204b6471ca43848ed890ed7ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/test-00000-of-00001.parquet:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a70612137d0343e4acbfeeb416689cf0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/1659083 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "340d8b846dbd4a29ae64fd8704af0872"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/520 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa73af0a02dc42f0b0a8a4f559fec3df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/2507 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4c2bfa2a4cc4ed681c9130d8df31d60"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2507 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db539c7ef57243029ae0572b03c52373"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== First 5 examples from IITB English-Hindi (test) ===\n",
            "\n",
            "Example 1:\n",
            "SRC: A black box in your car?\n",
            "REF: आपकी कार में ब्लैक बॉक्स?\n",
            "\n",
            "Example 2:\n",
            "SRC: As America's road planners struggle to find the cash to mend a crumbling highway system, many are beginning to see a solution in a little black box that fits neatly by the dashboard of your car.\n",
            "REF: जबकि अमेरिका के सड़क योजनाकार, ध्वस्त होते हुए हाईवे सिस्टम को सुधारने के लिए धन की कमी से जूझ रहे हैं, वहीं बहुत-से लोग इसका समाधान छोटे से ब्लैक बॉक्स में देख रहे हैं, जो आपकी कार के डैशबोर्ड पर सफ़ाई से फिट हो जाता है।\n",
            "\n",
            "Example 3:\n",
            "SRC: The devices, which track every mile a motorist drives and transmit that information to bureaucrats, are at the center of a controversial attempt in Washington and state planning offices to overhaul the outdated system for funding America's major roads.\n",
            "REF: यह डिवाइस, जो मोटर-चालक द्वारा वाहन चलाए गए प्रत्येक मील को ट्रैक करती है तथा उस सूचना को अधिकारियों को संचारित करती है, आजकल अमेरिका की प्रमुख सड़कों का वित्त-पोषण करने के लिए पुराने हो चुके सिस्टम का जीर्णोद्धार करने के लिए वाशिंगटन और राज्य नियोजन कार्यालय के लिए एक विवादास्पद प्रयास का मुद्दा बन चुका है।\n",
            "\n",
            "Example 4:\n",
            "SRC: The usually dull arena of highway planning has suddenly spawned intense debate and colorful alliances.\n",
            "REF: आम तौर पर हाईवे नियोजन जैसा उबाऊ काम भी अचानक गहन बहस तथा जीवंत गठबंधनों का मुद्दा बन गया है।\n",
            "\n",
            "Example 5:\n",
            "SRC: Libertarians have joined environmental groups in lobbying to allow government to use the little boxes to keep track of the miles you drive, and possibly where you drive them - then use the information to draw up a tax bill.\n",
            "REF: आपने द्वारा ड्राइव किए गए मील, तथा संभवतः ड्राइव किए गए स्थान का विवरण रखने - और फिर इस सूचना का उपयोग टैक्स बिल तैयार करने के लिए - सरकार को इन ब्लैक बॉक्स का उपयोग करने की अनुमति देने के पक्ष में समर्थन जुटाने के लिए लिबरेटेरियन पर्यावरणीय समूहों के साथ मिल गए हैं।\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load dataset\n",
        "\n",
        "print(f\"Loading dataset: {DATASET_ID} [{SPLIT}]\")\n",
        "ds = load_dataset(DATASET_ID, split=SPLIT)\n",
        "\n",
        "def extract_pair(example):\n",
        "    tr = example[\"translation\"]\n",
        "    return {\"src\": tr[\"en\"].strip(), \"ref\": tr[\"hi\"].strip()}\n",
        "\n",
        "ds = ds.map(extract_pair, remove_columns=ds.column_names)\n",
        "\n",
        "# Show first 5 raw examples\n",
        "print(\"\\n=== First 5 examples from IITB English-Hindi (test) ===\")\n",
        "for i in range(min(5, len(ds))):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(\"SRC:\", ds[i][\"src\"])\n",
        "    print(\"REF:\", ds[i][\"ref\"])\n",
        "\n",
        "# Safety check (we want full test)\n",
        "if EVAL_SIZE is not None and EVAL_SIZE < len(ds):\n",
        "    raise ValueError(\"EVAL_SIZE must be None to evaluate the entire test dataset.\")\n",
        "\n",
        "srcs = ds[\"src\"]\n",
        "refs = ds[\"ref\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8wYIsi4D54D",
        "outputId": "e4c686f3-0528-4930-c95c-de12f80bb4fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Translating… (full test set)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [04:37<00:00,  6.94s/it]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Translation (beam search)\n",
        "\n",
        "@torch.inference_mode()\n",
        "def translate_batch(texts):\n",
        "    enc = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    for k in enc:\n",
        "        enc[k] = enc[k].to(DEVICE)\n",
        "    gen = model.generate(\n",
        "        **enc,\n",
        "        max_new_tokens=MAX_NEW_TOKENS,\n",
        "        num_beams=NUM_BEAMS,\n",
        "        length_penalty=LENGTH_PENALTY,\n",
        "        no_repeat_ngram_size=NO_REPEAT_NGRAM,\n",
        "        use_cache=True,\n",
        "    )\n",
        "    out = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "    return [o.strip() for o in out]\n",
        "\n",
        "print(\"\\nTranslating… (full test set)\")\n",
        "hyps = []\n",
        "for i in tqdm(range(0, len(srcs), BATCH_SIZE)):\n",
        "    batch_src = srcs[i:i+BATCH_SIZE]\n",
        "    hyps.extend(translate_batch(batch_src))\n",
        "\n",
        "assert len(hyps) == len(refs)\n",
        "\n",
        "\n",
        "# Light normalization (Hindi)\n",
        "#is to clean and standardize Hindi text so that small, meaningless differences\n",
        "#(like spacing or punctuation inconsistencies) don’t unfairly affect evaluation metrics such as BLEU\n",
        "\n",
        "def normalize_hi(s: str) -> str:\n",
        "    s = s.strip()\n",
        "    # unify danda/full stop spacing and collapse spaces\n",
        "    s = s.replace(\" ।\", \"।\").replace(\" .\", \".\")\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "hyps = [normalize_hi(x) for x in hyps]\n",
        "refs = [normalize_hi(x) for x in refs]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_english_sentences = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"This is an example sentence.\",\n",
        "    \"Machine translation is interesting.\",\n",
        "    \"Ruksad loves to learn machine learning\"\n",
        "]\n",
        "\n",
        "hindi_translations = translate_batch(my_english_sentences)\n",
        "\n",
        "for original, translated in zip(my_english_sentences, hindi_translations):\n",
        "    print(f\"English: {original}\")\n",
        "    print(f\"Hindi: {translated}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhWZAV-U3Qpb",
        "outputId": "031b0346-d28f-4538-db98-0b56cdfc8010"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: Hello, how are you?\n",
            "Hindi: हैलो, तुम कैसे हो?\n",
            "\n",
            "English: This is an example sentence.\n",
            "Hindi: यह एक उदाहरण वाक्य है ।\n",
            "\n",
            "English: Machine translation is interesting.\n",
            "Hindi: मशीन अनुवाद दिलचस्प है.\n",
            "\n",
            "English: Ruksad loves to learn machine learning\n",
            "Hindi: रुडड मशीन सीखने के लिए प्यार करता है\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-V5csVQ0EaSz",
        "outputId": "66e7ba38-db10-40bb-fb2a-e829d820dd83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Computing corpus BLEU (sacrebleu)…\n",
            "\n",
            "=== Corpus BLEU (EN→HI, IITB full test) ===\n",
            "BLEU 13a : 9.79\n",
            "BLEU intl: 10.27  (recommended for Indic)\n",
            "\n",
            "Scoring sentences (sentence-level BLEU, intl)…\n",
            "\n",
            "=== Top 5 correct examples (by sentence BLEU, intl) ===\n",
            "\n",
            "[1] BLEU=78.25\n",
            "SRC: This project is a key element of energy security of the whole European continent.\n",
            "REF: यह परियोजना पूरे यूरोपीय महाद्वीप की ऊर्जा सुरक्षा का एक मुख्य तत्व है।\n",
            "HYP: यह परियोजना पूरे यूरोपीय महाद्वीप की ऊर्जा सुरक्षा का एक प्रमुख तत्व है।\n",
            "\n",
            "[2] BLEU=75.98\n",
            "SRC: What was the cause of death?\n",
            "REF: मौत का कारण क्या था?\n",
            "HYP: मृत्यु का कारण क्या था?\n",
            "\n",
            "[3] BLEU=74.48\n",
            "SRC: Someone in the German parliament says we should build a German Google.\n",
            "REF: जर्मन संसद में किसी ने कहा की हमे एक जर्मन गूगल का निर्माण करना चाहिए।\n",
            "HYP: जर्मन संसद में किसी ने कहा कि हम एक जर्मन गूगल का निर्माण करना चाहिए।\n",
            "\n",
            "[4] BLEU=70.17\n",
            "SRC: Frontier has gone the furthest in this area, though.\n",
            "REF: Frontier हालांकि, इस क्षेत्र में आगे चला गया है.\n",
            "HYP: हालांकि, इस क्षेत्र में सबसे आगे चला गया है.\n",
            "\n",
            "[5] BLEU=67.32\n",
            "SRC: What about the first?\n",
            "REF: पहले वाले के बारे में क्या?\n",
            "HYP: पहले के बारे में क्या?\n",
            "\n",
            "=== Bottom 5 bad-performing examples (by sentence BLEU, intl) ===\n",
            "\n",
            "[1] BLEU=0.00\n",
            "SRC: Wonks call it a mileage-based user fee.\n",
            "REF: वॉन्क्स इसे माइलेज-आधारित उपयोगकर्ता शुल्क कहते हैं।\n",
            "HYP: यह एक मील की दूरी पर उपयोक्ता कैश कॉल करता है.\n",
            "\n",
            "[2] BLEU=0.00\n",
            "SRC: Presenting CAT's trendy footwear and clothing collection.\n",
            "REF: कैट की ट्रैडी फुटवियर और परिधानों की सिलेक्शन पेश\n",
            "HYP: सी. टी. वी.\n",
            "\n",
            "[3] BLEU=0.00\n",
            "SRC: CAT is a brand that has been offering strong, durable, beautiful and high quality products for the past one hundred years.\n",
            "REF: कैट एक ऐसा ब्राड है जो पिछले सौ साल से मजबूत, टिकाऊ, सुंदर और बेहतरीन उत्पाद पेश कर रहा है।\n",
            "HYP: सी. टी. वी.\n",
            "\n",
            "[4] BLEU=0.00\n",
            "SRC: MRF has been awarded the JD Power Award, for the tenth time in a row.\n",
            "REF: एमआरएफ लगातार दसवीं बार जेडी पावर पुरस्कार से सम्मानित\n",
            "HYP: MRF को JD शक्‍ति Aagad, एक पंक्ति में दसवें समय के लिए दिया गया है.\n",
            "\n",
            "[5] BLEU=0.00\n",
            "SRC: Frontier Airlines to charge for carry-on baggage\n",
            "REF: Frontier एयरलाइंस कैरी - ऑन बैगेज का शुल्क लगाएंगे\n",
            "HYP: ले जाने के लिए आधिकारिक एयरलाइन\n",
            "\n",
            "Saved to: eval_outputs_en_hi_iitb_full_bleu/ (sources.txt, references.txt, hypotheses.txt)\n",
            "\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Corpus BLEU (13a and intl)\n",
        "\n",
        "print(\"\\nComputing corpus BLEU (sacrebleu)…\")\n",
        "bleu_13a  = sacrebleu.corpus_bleu(hyps, [refs], tokenize=\"13a\")\n",
        "bleu_intl = sacrebleu.corpus_bleu(hyps, [refs], tokenize=\"intl\")\n",
        "\n",
        "print(\"\\n=== Corpus BLEU (EN→HI, IITB full test) ===\")\n",
        "print(f\"BLEU 13a : {bleu_13a.score:.2f}\")\n",
        "print(f\"BLEU intl: {bleu_intl.score:.2f}  (recommended for Indic)\")\n",
        "\n",
        "\n",
        "# Sentence-level ranking (BLEU intl)\n",
        "\n",
        "print(\"\\nScoring sentences (sentence-level BLEU, intl)…\")\n",
        "sent_bleu_scores = [\n",
        "    sacrebleu.sentence_bleu(h, [r], tokenize=\"intl\").score\n",
        "    for h, r in zip(hyps, refs)\n",
        "]\n",
        "\n",
        "def topk_indices(scores, k, reverse=True):\n",
        "    return sorted(range(len(scores)), key=lambda i: scores[i], reverse=reverse)[:k]\n",
        "\n",
        "K = 5\n",
        "best_idx = topk_indices(sent_bleu_scores, K, reverse=True)\n",
        "worst_idx = topk_indices(sent_bleu_scores, K, reverse=False)\n",
        "\n",
        "def pretty_show(indices, title):\n",
        "    print(f\"\\n=== {title} (by sentence BLEU, intl) ===\")\n",
        "    for rank, i in enumerate(indices, 1):\n",
        "        print(f\"\\n[{rank}] BLEU={sent_bleu_scores[i]:.2f}\")\n",
        "        print(f\"SRC: {srcs[i]}\")\n",
        "        print(f\"REF: {refs[i]}\")\n",
        "        print(f\"HYP: {hyps[i]}\")\n",
        "\n",
        "pretty_show(best_idx, \"Top 5 correct examples\")\n",
        "pretty_show(worst_idx, \"Bottom 5 bad-performing examples\")\n",
        "\n",
        "\n",
        "# Save outputs (optional)\n",
        "\n",
        "out_dir = \"eval_outputs_en_hi_iitb_full_bleu\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "with open(os.path.join(out_dir, \"sources.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(srcs))\n",
        "with open(os.path.join(out_dir, \"references.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(refs))\n",
        "with open(os.path.join(out_dir, \"hypotheses.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(hyps))\n",
        "\n",
        "print(f\"\\nSaved to: {out_dir}/ (sources.txt, references.txt, hypotheses.txt)\")\n",
        "print(\"\\nDone.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cf35cad03f4643d7b21cc9ac7e6a6929",
            "aedec5fba79f4909bd79ee07692e74b0",
            "ad8462465f154869ae7d9942c2f72a00",
            "40d5926537f14118977d50e02c2608a2",
            "31ed1d3b918941209bf6e1e0104d2747",
            "b05abd7e881e4efc90b63b516ea5a947",
            "8c51e62e46144b5f8756dcf048b3d6dc",
            "63a89eb0ed9847e3968648bc2289db50",
            "b2aed4fd2c384ffcaba9e48201121236",
            "1e51732528b1442681d8379016f66303",
            "2b8885295fcc48ab96ca9c9461eb7ff0"
          ]
        },
        "id": "SIJxkkyaBoBy",
        "outputId": "1d85f312-75cd-4bfd-80fb-e2666cbd17ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: Helsinki-NLP/opus-mt-en-hi on cuda (fp16=True)\n",
            "Loading dataset: cfilt/iitb-english-hindi [test]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2507 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf35cad03f4643d7b21cc9ac7e6a6929"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== First 5 examples from IITB English-Hindi (test) ===\n",
            "\n",
            "Example 1:\n",
            "SRC: A black box in your car?\n",
            "REF: आपकी कार में ब्लैक बॉक्स?\n",
            "\n",
            "Example 2:\n",
            "SRC: As America's road planners struggle to find the cash to mend a crumbling highway system, many are beginning to see a solution in a little black box that fits neatly by the dashboard of your car.\n",
            "REF: जबकि अमेरिका के सड़क योजनाकार, ध्वस्त होते हुए हाईवे सिस्टम को सुधारने के लिए धन की कमी से जूझ रहे हैं, वहीं बहुत-से लोग इसका समाधान छोटे से ब्लैक बॉक्स में देख रहे हैं, जो आपकी कार के डैशबोर्ड पर सफ़ाई से फिट हो जाता है।\n",
            "\n",
            "Example 3:\n",
            "SRC: The devices, which track every mile a motorist drives and transmit that information to bureaucrats, are at the center of a controversial attempt in Washington and state planning offices to overhaul the outdated system for funding America's major roads.\n",
            "REF: यह डिवाइस, जो मोटर-चालक द्वारा वाहन चलाए गए प्रत्येक मील को ट्रैक करती है तथा उस सूचना को अधिकारियों को संचारित करती है, आजकल अमेरिका की प्रमुख सड़कों का वित्त-पोषण करने के लिए पुराने हो चुके सिस्टम का जीर्णोद्धार करने के लिए वाशिंगटन और राज्य नियोजन कार्यालय के लिए एक विवादास्पद प्रयास का मुद्दा बन चुका है।\n",
            "\n",
            "Example 4:\n",
            "SRC: The usually dull arena of highway planning has suddenly spawned intense debate and colorful alliances.\n",
            "REF: आम तौर पर हाईवे नियोजन जैसा उबाऊ काम भी अचानक गहन बहस तथा जीवंत गठबंधनों का मुद्दा बन गया है।\n",
            "\n",
            "Example 5:\n",
            "SRC: Libertarians have joined environmental groups in lobbying to allow government to use the little boxes to keep track of the miles you drive, and possibly where you drive them - then use the information to draw up a tax bill.\n",
            "REF: आपने द्वारा ड्राइव किए गए मील, तथा संभवतः ड्राइव किए गए स्थान का विवरण रखने - और फिर इस सूचना का उपयोग टैक्स बिल तैयार करने के लिए - सरकार को इन ब्लैक बॉक्स का उपयोग करने की अनुमति देने के पक्ष में समर्थन जुटाने के लिए लिबरेटेरियन पर्यावरणीय समूहों के साथ मिल गए हैं।\n",
            "\n",
            "Translating… (full test set)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:28<00:00,  1.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Computing corpus BLEU (sacrebleu)…\n",
            "\n",
            "=== Corpus BLEU (EN→HI, IITB full test) ===\n",
            "BLEU: 8.96\n",
            "\n",
            "Scoring sentences (sentence-level BLEU)…\n",
            "\n",
            "=== Top 5 correct examples (by sentence BLEU) ===\n",
            "\n",
            "[1] BLEU=100.00\n",
            "SRC: What was the cause of death?\n",
            "REF: मौत का कारण क्या था?\n",
            "HYP: मौत का कारण क्या था?\n",
            "\n",
            "[2] BLEU=69.31\n",
            "SRC: This project is a key element of energy security of the whole European continent.\n",
            "REF: यह परियोजना पूरे यूरोपीय महाद्वीप की ऊर्जा सुरक्षा का एक मुख्य तत्व है।\n",
            "HYP: यह परियोजना पूरे यूरोपीय महाद्वीप की ऊर्जा सुरक्षा का एक प्रमुख तत्व है ।\n",
            "\n",
            "[3] BLEU=67.32\n",
            "SRC: What about the first?\n",
            "REF: पहले वाले के बारे में क्या?\n",
            "HYP: पहले के बारे में क्या?\n",
            "\n",
            "[4] BLEU=63.89\n",
            "SRC: And I think about my father.\n",
            "REF: और मैं अपने पिता के बारे में सोचता हूँ।\n",
            "HYP: और मैं अपने पिता के बारे में सोचते हैं.\n",
            "\n",
            "[5] BLEU=60.04\n",
            "SRC: Share with us your thoughts in the comments below.\n",
            "REF: अपने विचार नीचे टिप्पणियों में हमारे साथ साझा करें।\n",
            "HYP: नीचे टिप्पणियों में हमारे साथ साझा करें.\n",
            "\n",
            "=== Bottom 5 bad-performing examples (by sentence BLEU) ===\n",
            "\n",
            "[1] BLEU=0.00\n",
            "SRC: Wonks call it a mileage-based user fee.\n",
            "REF: वॉन्क्स इसे माइलेज-आधारित उपयोगकर्ता शुल्क कहते हैं।\n",
            "HYP: यह एक मील की दूरी पर उपयोक्ता भुगतान कॉल करता है.\n",
            "\n",
            "[2] BLEU=0.00\n",
            "SRC: In Oregon, planners are experimenting with giving drivers different choices.\n",
            "REF: ऑरेगॉन में, योजनाकार चालकों को भिन्न-भिन्न विकल्प देने का प्रयोग कर रहे हैं।\n",
            "HYP: एक और मिसाल लीजिए ।\n",
            "\n",
            "[3] BLEU=0.00\n",
            "SRC: The Maharaja of the dynastic capital has arranged some magnificent cuisine from the majestic palaces.\n",
            "REF: खानदानी राजधानी के महाराज ने आपके लिए राजसी महलों के शानदार व्यंजनों का प्रबंध किया है।\n",
            "HYP: इस द्वीप की शान बहुत ही खूबसूरत थी ।\n",
            "\n",
            "[4] BLEU=0.00\n",
            "SRC: CAT is a brand that has been offering strong, durable, beautiful and high quality products for the past one hundred years.\n",
            "REF: कैट एक ऐसा ब्राड है जो पिछले सौ साल से मजबूत, टिकाऊ, सुंदर और बेहतरीन उत्पाद पेश कर रहा है।\n",
            "HYP: सी. टी.\n",
            "\n",
            "[5] BLEU=0.00\n",
            "SRC: CAT, in a way, is the blend of durability and lifestyle.\n",
            "REF: कैट एक तरह से मजबूती और लाइफ स्टाइल का मिलन है।\n",
            "HYP: सी. टी.\n",
            "\n",
            "Saved to: eval_outputs_en_hi_iitb_full_bleu/ (sources.txt, references.txt, hypotheses.txt)\n",
            "\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Fast IITB EN→HI evaluation with Helsinki-NLP/opus-mt-en-hi\n",
        "- Prints first 5 dataset examples\n",
        "- FULL test evaluation (no subsetting)\n",
        "- BLEU via sacrebleu (corpus + sentence-level)\n",
        "- 5 best / 5 worst examples by sentence BLEU\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import sacrebleu\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Config (tune these for speed/quality)\n",
        "\n",
        "MODEL_ID = \"Helsinki-NLP/opus-mt-en-hi\"\n",
        "DATASET_ID = \"cfilt/iitb-english-hindi\"\n",
        "SPLIT = \"test\"\n",
        "\n",
        "# Evaluate FULL test set\n",
        "EVAL_SIZE = None          # <-- full test; do not change unless you want a subset\n",
        "BATCH_SIZE = 64           # adjust for your hardware\n",
        "MAX_NEW_TOKENS = 96\n",
        "NUM_BEAMS = 1             # 1 = greedy (keep same behavior)\n",
        "NO_REPEAT_NGRAM = 0\n",
        "SEED = 42\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "USE_FP16 = DEVICE == \"cuda\"  # enable half-precision on GPU\n",
        "\n",
        "\n",
        "# Reproducibility & backend\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if DEVICE == \"cuda\":\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "# Load model & tokenizer\n",
        "\n",
        "print(f\"Loading model: {MODEL_ID} on {DEVICE} (fp16={USE_FP16})\")\n",
        "tokenizer = MarianTokenizer.from_pretrained(MODEL_ID)\n",
        "model = MarianMTModel.from_pretrained(MODEL_ID)\n",
        "if USE_FP16:\n",
        "    model = model.half()\n",
        "model = model.to(DEVICE).eval()\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "\n",
        "print(f\"Loading dataset: {DATASET_ID} [{SPLIT}]\")\n",
        "ds = load_dataset(DATASET_ID, split=SPLIT)\n",
        "\n",
        "def extract_pair(example):\n",
        "    tr = example[\"translation\"]\n",
        "    return {\"src\": tr[\"en\"].strip(), \"ref\": tr[\"hi\"].strip()}\n",
        "\n",
        "ds = ds.map(extract_pair, remove_columns=ds.column_names)\n",
        "\n",
        "# Show first 5 raw examples\n",
        "print(\"\\n=== First 5 examples from IITB English-Hindi (test) ===\")\n",
        "for i in range(5):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(\"SRC:\", ds[i][\"src\"])\n",
        "    print(\"REF:\", ds[i][\"ref\"])\n",
        "\n",
        "# (No subsetting; evaluate full test set)\n",
        "if EVAL_SIZE is not None and EVAL_SIZE < len(ds):\n",
        "    raise ValueError(\"EVAL_SIZE is not None; set it to None to evaluate the entire test dataset.\")\n",
        "\n",
        "srcs = ds[\"src\"]\n",
        "refs = ds[\"ref\"]\n",
        "\n",
        "# Batched translation\n",
        "\n",
        "@torch.inference_mode()\n",
        "def translate_batch(texts):\n",
        "    enc = tokenizer(\n",
        "        texts, return_tensors=\"pt\", padding=True, truncation=True\n",
        "    )\n",
        "    # Move to device\n",
        "    for k in enc:\n",
        "        enc[k] = enc[k].to(DEVICE)\n",
        "    gen = model.generate(\n",
        "        **enc,\n",
        "        max_new_tokens=MAX_NEW_TOKENS,\n",
        "        num_beams=NUM_BEAMS,\n",
        "        length_penalty=1.0,\n",
        "        no_repeat_ngram_size=NO_REPEAT_NGRAM,\n",
        "        use_cache=True,\n",
        "    )\n",
        "    out = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "    return [o.strip() for o in out]\n",
        "\n",
        "print(\"\\nTranslating… (full test set)\")\n",
        "hyps = []\n",
        "for i in tqdm(range(0, len(srcs), BATCH_SIZE)):\n",
        "    batch_src = srcs[i:i+BATCH_SIZE]\n",
        "    hyps.extend(translate_batch(batch_src))\n",
        "\n",
        "assert len(hyps) == len(refs)\n",
        "\n",
        "\n",
        "# Corpus BLEU only\n",
        "\n",
        "print(\"\\nComputing corpus BLEU (sacrebleu)…\")\n",
        "bleu = sacrebleu.corpus_bleu(hyps, [refs])\n",
        "\n",
        "print(\"\\n=== Corpus BLEU (EN→HI, IITB full test) ===\")\n",
        "print(f\"BLEU: {bleu.score:.2f}\")\n",
        "\n",
        "\n",
        "# Sentence-level ranking (BLEU)\n",
        "\n",
        "print(\"\\nScoring sentences (sentence-level BLEU)…\")\n",
        "# sacrebleu.sentence_bleu returns an object with .score\n",
        "sent_bleu_scores = [sacrebleu.sentence_bleu(h, [r]).score for h, r in zip(hyps, refs)]\n",
        "\n",
        "def topk_indices(scores, k, reverse=True):\n",
        "    return sorted(range(len(scores)), key=lambda i: scores[i], reverse=reverse)[:k]\n",
        "\n",
        "K = 5\n",
        "best_idx = topk_indices(sent_bleu_scores, K, reverse=True)\n",
        "worst_idx = topk_indices(sent_bleu_scores, K, reverse=False)\n",
        "\n",
        "def pretty_show(indices, title):\n",
        "    print(f\"\\n=== {title} (by sentence BLEU) ===\")\n",
        "    for rank, i in enumerate(indices, 1):\n",
        "        print(f\"\\n[{rank}] BLEU={sent_bleu_scores[i]:.2f}\")\n",
        "        print(f\"SRC: {srcs[i]}\")\n",
        "        print(f\"REF: {refs[i]}\")\n",
        "        print(f\"HYP: {hyps[i]}\")\n",
        "\n",
        "pretty_show(best_idx, \"Top 5 correct examples\")\n",
        "pretty_show(worst_idx, \"Bottom 5 bad-performing examples\")\n",
        "\n",
        "\n",
        "# Save outputs (optional)\n",
        "\n",
        "out_dir = \"eval_outputs_en_hi_iitb_full_bleu\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "with open(os.path.join(out_dir, \"sources.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(srcs))\n",
        "with open(os.path.join(out_dir, \"references.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(refs))\n",
        "with open(os.path.join(out_dir, \"hypotheses.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(hyps))\n",
        "\n",
        "print(f\"\\nSaved to: {out_dir}/ (sources.txt, references.txt, hypotheses.txt)\")\n",
        "print(\"\\nDone.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
